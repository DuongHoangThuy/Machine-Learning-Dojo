{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 3: Multi-class Classification and Neural Networks\n",
    "\n",
    "## Giới thiệu\n",
    "\n",
    "Trong bài tập này, chúng ta sẽ cài đặt mô hình **one-vs-all** logistic regression và neural networks cho nhận dạng chữ số viết tay. Dữ liệu dùng cho bài tập này gồm hai file:\n",
    "\n",
    "- *ex3data1.mat* là dữ liệu huấn luyện cho bài toán nhận dạng chữ số viết tay.\n",
    "- *ex3weights.mat* là các trọng số ban đầu cho phần bài tập về neural networks.\n",
    "\n",
    "## 1. Multi-class Classification\n",
    "\n",
    "Trong bài tập này, bạn sẽ học cách sử dụng logistic regression và neural networks để nhận dạng chữ số viết tay (từ 0 đến 9). Nhận dạng chữ số viết tay được ứng dụng nhiều trong thực tế, ví dụ: nhận dạng mã bưu điện trên các phong bì hay nhận dạng số tiền trên bank checks.\n",
    "\n",
    "Trong phần đầu của bài tập này, chúng ta sẽ mở rộng cài đặt logistic regression trong bài tập số 2 và áp dụng phương pháp **one-vs-all** cho phân lớp.\n",
    "\n",
    "### 1.1 Dữ liệu\n",
    "\n",
    "File dữ liệu ```ex3data1.mat``` có 5000 ví dụ về chữ số viết tay. File này được lưu ở định dạng ```.mat``` của Octave/Matlab. Để đọc dữ liệu trong python, ta sẽ sử dụng hàm ```scipy.io.loadmat``` (Tham khảo thêm tại [scipy.io](http://docs.scipy.org/doc/scipy/reference/io.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5000, 400)\n",
      "Shape of y: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mat_dict = loadmat('ex3data1.mat')\n",
    "X = mat_dict['X']\n",
    "y = mat_dict['y']\n",
    "m = X.shape[0]\n",
    "\n",
    "sys.stdout.write('Shape of X: ')\n",
    "print X.shape\n",
    "sys.stdout.write('Shape of y: ')\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn code trên đọc dữ liệu vào các biến ```X``` và ```y``` kiểu ```numpy.ndarray```. Trong đó, ```y``` chứa các nhãn của các example trong dữ liệu. Vì dữ liệu được biến đổi thành dạng tương thích với Octave/Matlab nên chữ số \"0\" sẽ được biểu diễn bằng nhãn \"10\" trong tập dữ liệu.\n",
    "\n",
    "Mỗi ảnh trong tập dữ liệu được lưu dưới dạng 20x20, nên mỗi example là một vector với số chiều là 400. Nếu ta coi mỗi vector là các vector cột thì ```X``` sẽ được biểu diễn dưới dạng sau đây.\n",
    "\n",
    "$$X=\\left[\n",
    "\\begin{array}{c}\n",
    "-\\left(x^{(1)}\\right)^T-\\\\\n",
    "-\\left(x^{(2)}\\right)^T-\\\\\n",
    "\\vdots\\\\\n",
    "-\\left(x^{(m)}\\right)^T-\\\\\n",
    "\\end{array}\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Visualizing the data\n",
    "\n",
    "Chúng ta sẽ bắt đầu bằng việc visualize dữ liệu. Trong phần này, chúng ta sẽ chọn ngẫu nhiên 100 examples từ dữ liệu và hiển thị các số tương ứng dưới dạng ảnh pixel 20x20 nền xám.\n",
    "\n",
    "*Note*: Vì đây không phải là nội dung chính của bài học nên tạm thời tôi sẽ bỏ qua phần này để làm các phần quan trọng trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Data ...\n"
     ]
    }
   ],
   "source": [
    "print 'Visualizing Data ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cài đặt logistic regression với vectorization\n",
    "\n",
    "Để tăng tốc độ huấn luyện trên dữ liệu, trong phần này, chúng ta sẽ cài đặt logistic regression sử dụng kỹ thuật vectorization vì các thao tác trên vector hay ma trận trong python numpy hiệu quả hơn rất nhiều so với sử dụng vòng lặp.\n",
    "\n",
    "#### 1.3.1 Vectorizing the cost function\n",
    "\n",
    "Chúng ta sẽ cài đặt hàm cost với kỹ thuật vectorization. Như chúng ta đã biết, trong thuật toán logistic regression, hàm cost function (phiên bản unregularized) được tính bằng công thức sau.\n",
    "\n",
    "$$J(\\theta) =\\frac{1}{m}\\sum_{i=1}^{m}\\left[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\right]$$\n",
    "\n",
    "#### 1.3.2 Vectorizing the gradient\n",
    "\n",
    "Trong logistic regression, gradient của cost function là 1 vector có cùng độ dài với $\\theta$ trong đó phần tử $j^{th}$ (với $j=0,1,\\cdots,n$ được định nghĩa như sau:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}$$\n",
    "\n",
    "#### 1.3.3 Regularized logistic regression\n",
    "Chú ý rằng, hàm cost function cho phiên bản regularization có công thức như sau:\n",
    "$$J(\\theta) =\\frac{1}{m}\\sum_{i=1}^{m}\\left[-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$$\n",
    "\n",
    "Chú ý rằng, chúng ta sẽ không áp dụng regularization cho tham số $\\theta_0$. Vì thế, gradient cho cost functon là một vector trong đó phần tử thứ $j^{th}$ được định nghĩa như sau:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0}=\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)}) - y{(i)}\\right)x_j^{(i)} \\text{   với   } j=0$$\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)}) - y{(i)}\\right)x_j^{(i)}\\right) + \\frac{\\lambda}{m}\\theta_j \\text{    với    } j \\geq 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z can be np.ndarray, np.matrix, or scalar\n",
    "def sigmoid(z):\n",
    "    return 1 / ( 1 + np.exp(-z) ) \n",
    "\n",
    "def cost_function(theta, X, y, _lambda):\n",
    "    m = y.shape[0]\n",
    "    J = 0\n",
    "    hx = sigmoid( np.dot(X, theta) ) # hx.shape m x 1\n",
    "    y_flat = y.flatten() # for element-wise operators\n",
    "    J = np.sum( -y_flat * np.log(hx) - (1-y_flat) * np.log(1-hx) )/m\n",
    "    J += _lambda * (theta[1:] ** 2).sum()/(2*m)\n",
    "    return J\n",
    "\n",
    "# theta.shape (n,) (ndim=1)\n",
    "# X.shape = m x n (ndim = 2)\n",
    "# y.shape = m x 1 (ndim =2)\n",
    "# return gradient vector with shape (n,) (ndim=1)\n",
    "def gradient(theta, X, y, _lambda):\n",
    "    m = y.shape[0]\n",
    "    n = theta.shape[0]\n",
    "    grad = np.zeros( n )\n",
    "    hx = sigmoid( np.dot(X, theta) ) # hx.shape (m, ) ndim = 1\n",
    "    hx = hx.reshape( (m, 1) )\n",
    "    grad = np.dot(X.T, hx - y) / m\n",
    "    \n",
    "    grad = grad.flatten()\n",
    "    temp = np.array(theta)\n",
    "    temp[0] = 0\n",
    "    grad += _lambda * temp / m\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 One-vs-all Classification\n",
    "Trong section này, chúng ta sẽ cài đặt mô hình phân lớp one-vs-all bằng cách huấn luyện một số bộ phân lớp với regularized logistic regression. Mỗi bộ phân lớp tương ứng với một class trong tập dữ liệu huấn luyện.\n",
    "\n",
    "Đầu ra của hàm dưới đây là các tham số cho các bộ phân lớp, và được lưu trong một ma trận $\\Theta \\in \\mathbb{R}^{K\\times(N+1)}$. Ở đây, mỗi dòng của ma trận $\\Theta$ tương ứng với các tham số của một bộ phân lớp cho lớp tương ứng.\n",
    "\n",
    "Khi huấn luyện bộ phân lớp cho lớp $k \\in \\{1,\\cdots,K\\}$, chúng ta cần một vector chứa các nhãn $y$, trong đó $y_j\\in0,1$ để xác định instance thứ *j-th* thuộc lớp $k$ ($y_j=1$) hay hoặc các lớp khác ($y_j=0$). Ta sẽ dùng *logical array* cho công việc này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs, fmin\n",
    "\n",
    "def oneVsAll(X, y, num_labels, _lambda):\n",
    "    m = X.shape[0] # number of training examples\n",
    "    n = X.shape[1] # number of features\n",
    "    all_theta = np.zeros( (num_labels, n+1), dtype=np.float64 )\n",
    "    # add ones to X data matrix\n",
    "    X_new = np.concatenate( (np.ones( (m,1) ), X), axis=1 )\n",
    "    \n",
    "    for c in xrange(1, num_labels+1):\n",
    "        initial_theta = np.zeros( n+1, dtype=np.float64 )\n",
    "        y_lb = np.array( (y == c), dtype=np.int )\n",
    "        theta = fmin_bfgs(cost_function, initial_theta,\n",
    "                          fprime=gradient, args=(X_new, y_lb, _lambda),\n",
    "                          disp=False)\n",
    "        all_theta[c-1] = theta.reshape( (1, n+1) )\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 One-vs-all Prediction\n",
    "Sau khi đã huấn luyện xong bộ phân lớp với phương pháp **one-vs-all**, chúng ta sẽ dự đoán các chữ số viết tay cho mỗi ảnh cho trước. Với mỗi đầu vào, chúng ta sẽ dùng các bộ phân lớp đã học để tính \"xác suất\" ảnh đó thuộc mỗi lớp, sau đó chúng ta sẽ chọn ra lớp mà bộ phân lớp tương ứng trả về giá trị xác suất lớn nhất.\n",
    "\n",
    "Trong cài đặt dưới đây, tôi sử dụng hàm ```np.argmax``` để trả về indexes của giá trị lớn nhất trong một chiều cho trước của dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_theta.shape = num_labels x (n+1)\n",
    "# X has shape: m x n\n",
    "def predictOneVsAll(all_theta, X):\n",
    "    m = X.shape[0]\n",
    "    num_labels = all_theta.shape[0]\n",
    "    p = np.zeros( (m, 1), dtype=np.float32 )\n",
    "    \n",
    "    X_new = np.concatenate( (np.ones( (m,1) ), X), axis=1 )\n",
    "    prob = sigmoid( np.dot(X_new, all_theta.T) )\n",
    "    p = np.argmax( prob, axis=1 ) + 1\n",
    "    return p.reshape( (m, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Tính độ chính xác của mô hình trên dữ liệu huấn luyện\n",
    "Ta sẽ tính độ chính của mô hình thu được trên dữ liệu huấn luyện. Độ chính xác thu được là 96.48%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training One-vs-All Logistic Regression...\n"
     ]
    }
   ],
   "source": [
    "print 'Training One-vs-All Logistic Regression...'\n",
    "num_labels = 10\n",
    "_lambda = 0.1\n",
    "all_theta = oneVsAll(X, y, num_labels, _lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi có các tham số của các bộ phân lớp. Ta sẽ đưa ra dự đoán trên tập dữ liệu training và đưa ra độ chính xác của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (with fmin_bfgs): 96.48\n"
     ]
    }
   ],
   "source": [
    "pred = predictOneVsAll(all_theta, X)\n",
    "print('Training accuracy (with fmin_bfgs): %s' % \n",
    "      ( 100 * (pred == y).mean() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Networks\n",
    "\n",
    "Trong phần trước của bài tập này, chúng ta đã cài đặt **multi-class logistic regression** để nhận dạng chữ số viết tay. Tuy nhiên logistic regression không thể sinh ra các **hypothesis** phức tạp hơn vì nó chỉ là một bộ phân lớp tuyến tính.\n",
    "\n",
    "Trong phần này, chúng ta sẽ sử dụng **neural networks** để nhận dạng chữ số viết tay sử dụng cùng tập dữ liệu ở trên. Mạng neural sẽ có khả năng biểu diễn các mô hình phức tạp và các hypothesis phi tuyến tính. Trong bài tập này, chúng ta sẽ sử dụng các tham số đã được huấn luyện. Mục tiêu của bài tập là cài đặt thuật toán **feedforward propagation** để sử dụng các trọng số cho việc dự đoán. Trong các bài tập tiếp theo, chúng ta sẽ cài đặt thuật toán **backpropagation** để học các tham số của mạng neural.\n",
    "\n",
    "### 2.1 Model representation\n",
    "\n",
    "Hình 2 thể hiện mạng neural chúng ta sử dụng. Mạng neural này bao gồm 3 tầng: tầng input, 1 tầng ẩn, và 1 tầng output.\n",
    "\n",
    "Các tham số của mạng neural $(\\Theta_{(1)},\\Theta_{(2)})$ được cung cấp sẵn và lưu trong file ```ex3weights.mat```. Các tham số này có các chiều tương thích với số units của mỗi tầng. Ở đây, chúng ta có 25 units ở tầng thứ 2 (tầng ẩn) và 10 units ở tầng output (tương ứng với 10 digit classes).\n",
    "\n",
    "<img src=\"neural_network_model.png\" alt=\"Drawing\" style=\"width:350px;\"/>\n",
    "\n",
    "Đoạn code dưới đây sẽ đọc vào dữ liệu và các tham số từ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Theta1 and Theta2:\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "mat_dict = loadmat('ex3data1.mat')\n",
    "X = mat_dict['X']\n",
    "y = mat_dict['y']\n",
    "m = X.shape[0]\n",
    "\n",
    "para_dict = loadmat('ex3weights.mat')\n",
    "Theta1 = para_dict['Theta1']\n",
    "Theta2 = para_dict['Theta2']\n",
    "\n",
    "print('Shape of Theta1 and Theta2:')\n",
    "print Theta1.shape\n",
    "print Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Thuật toán Feedforward Propagation và dự đoán\n",
    "\n",
    "Trong phần này, chúng ta sẽ cài đặt thuật toán **Feedforward Propagation** để đưa ra dự đoán nhãn của mỗi ảnh trong dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X: 5000 * 400\n",
    "# Theta1: 25 x 401\n",
    "# Theta2: 10 * 26\n",
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0]\n",
    "    X_new = np.concatenate( (np.ones( (m,1) ), X), axis=1 ) # 5000 x 401\n",
    "    A2 = sigmoid( np.dot(X_new, Theta1.T) ) # 5000 x 25\n",
    "    A2 = np.concatenate( ( np.ones( (m,1) ), A2 ), axis=1 ) # 5000 x 26\n",
    "    A3 = sigmoid( np.dot(A2, Theta2.T) ) # 5000 x 10\n",
    "    p = np.argmax( A3, axis=1 ) + 1\n",
    "    return p.reshape( (m, 1) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áp dụng hàm ```predict``` đã cài đặt ở trên, chúng ta sẽ tính độ chính xác của mô hình trên dữ liệu huấn luyện. Độ chính xác trên dữ liệu huấn luyện là 97.52%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (with neural networks): 97.52\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X)\n",
    "print('Training accuracy (with neural networks): %s' % \n",
    "      ( 100 * (pred == y).mean() ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
