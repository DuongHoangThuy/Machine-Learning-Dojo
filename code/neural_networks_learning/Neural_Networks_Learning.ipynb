{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 4: Neural Network Learning\n",
    "\n",
    "## Giới thiệu\n",
    "\n",
    "Trong bài tập này, chúng ta sẽ cài đặt thuật toán **backpropagation** cho mạng neural và áp dụng vào bài toán nhận dạng chữ số viết tay. Dữ liệu dùng cho bài tập này gồm hai file:\n",
    "\n",
    "- *ex4data1.mat* là dữ liệu huấn luyện cho bài toán nhận dạng chữ số viết tay.\n",
    "- *ex4weights.mat* lưu trữ các tham số của neural networks.\n",
    "\n",
    "Bạn có thể xem cài đặt chi tiết của bài tập này trong một script duy nhất tại [đây](https://github.com/minhpqn/Machine-Learning-Dojo/blob/master/code/neural_networks_learning/Neural_Networks_Learning.py).\n",
    "\n",
    "## 1. Neural Networks\n",
    "\n",
    "Trong [bài tập trước](https://github.com/minhpqn/Machine-Learning-Dojo/blob/master/code/multi_class_nn/Multi-class_NN.ipynb), chúng ta đã cài đặt thuật toán **feedforward propagation** cho mạng neural và sử dụng nó để nhận dạng chữ số viết tay với các tham số được cho trước. Trong bài tập này, chúng ta sẽ cài đặt thuật toán **backpropagation** để học các tham số cho mạng neural.\n",
    "\n",
    "### 1.1 Visualizing the data\n",
    "\n",
    "File dữ liệu ```ex4data1.mat``` có 5000 ví dụ về chữ số viết tay. File này được lưu ở định dạng ```.mat``` của Octave/Matlab. Để đọc dữ liệu trong python, ta sẽ sử dụng hàm ```scipy.io.loadmat``` (Tham khảo thêm tại [scipy.io](http://docs.scipy.org/doc/scipy/reference/io.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5000, 400)\n",
      "Shape of y: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mat_dict = loadmat('ex4data1.mat')\n",
    "X = mat_dict['X']\n",
    "y = mat_dict['y']\n",
    "m = X.shape[0]\n",
    "\n",
    "sys.stdout.write('Shape of X: ')\n",
    "print X.shape\n",
    "sys.stdout.write('Shape of y: ')\n",
    "print y.shape\n",
    "\n",
    "# Flatten y to simplify computation, now y has shape (5000,) and 1D\n",
    "y = y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn code trên đọc dữ liệu vào các biến ```X``` và ```y``` kiểu ```numpy.ndarray```. Trong đó, ```y``` chứa các nhãn của các example trong dữ liệu. Vì dữ liệu được biến đổi thành dạng tương thích với Octave/Matlab nên chữ số \"0\" sẽ được biểu diễn bằng nhãn \"10\" trong tập dữ liệu.\n",
    "\n",
    "Mỗi ảnh trong tập dữ liệu được lưu dưới dạng 20x20, nên mỗi example là một vector với số chiều là 400. Nếu ta coi mỗi vector là các vector cột thì ```X``` sẽ được biểu diễn dưới dạng sau đây.\n",
    "\n",
    "$$X=\\left[\n",
    "\\begin{array}{c}\n",
    "-\\left(x^{(1)}\\right)^T-\\\\\n",
    "-\\left(x^{(2)}\\right)^T-\\\\\n",
    "\\vdots\\\\\n",
    "-\\left(x^{(m)}\\right)^T-\\\\\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Chúng ta sẽ bắt đầu bằng việc visualize dữ liệu. Trong phần này, chúng ta sẽ chọn ngẫu nhiên 100 examples từ dữ liệu và hiển thị các số tương ứng dưới dạng ảnh pixel 20x20 nền xám.\n",
    "\n",
    "*Note*: Vì đây không phải là nội dung chính của bài học nên tạm thời tôi sẽ bỏ qua phần này để làm các phần quan trọng trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Data ...\n"
     ]
    }
   ],
   "source": [
    "print 'Visualizing Data ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation\n",
    "\n",
    "Hình 2 thể hiện mạng neural chúng ta sử dụng. Mạng neural này bao gồm 3 tầng: tầng input, 1 tầng ẩn, và 1 tầng output.\n",
    "\n",
    "Các tham số của mạng neural $(\\Theta_{(1)},\\Theta_{(2)})$ được cung cấp sẵn và lưu trong file ```ex3weights.mat```. Các tham số này có các chiều tương thích với số units của mỗi tầng. Ở đây, chúng ta có 25 units ở tầng thứ 2 (tầng ẩn) và 10 units ở tầng output (tương ứng với 10 digit classes).\n",
    "\n",
    "<img src=\"neural_network_model.png\" alt=\"Drawing\" style=\"width:350px;\"/>\n",
    "\n",
    "Đoạn code dưới đây sẽ đọc vào dữ liệu và các tham số từ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Theta1 and Theta2:\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "para_dict = loadmat('ex4weights.mat')\n",
    "Theta1 = para_dict['Theta1']\n",
    "Theta2 = para_dict['Theta2']\n",
    "\n",
    "print('Shape of Theta1 and Theta2:')\n",
    "print Theta1.shape\n",
    "print Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feedforward và cost function\n",
    "\n",
    "Chúng ta sẽ cài đặt hàm cost và gradient cho mạng neural. Như chúng ta đã học trong [bài giảng](https://github.com/minhpqn/Machine-Learning-Dojo/blob/master/code/neural_networks_learning/Lecture9.pdf), hàm cost cho mạng neural (chưa sử dụng regularization) được tính như sau.\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\left[-y_k^{(i)}\\log((h_\\theta(x^{(i)}))_k)-(1-y_k^{(i)}) \\log(1-(h_\\theta(x^{(i)}))_k) \\right]$$\n",
    "\n",
    "Trong đó $h_\\theta(x^{(i)})$ được tính như trong hình 2, $K=10$ là tổng số labels trong dữ liệu. Chú ý rằng $h_\\theta(x^{(i)})_k=a_k^{(3)}$ là *activation* (đầu ra) của unit thứ $k$ của tầng output.\n",
    "\n",
    "Măc dù các nhãn ban đầu của dữ liệu là 1, 2, ..., 10, khi huấn luyện mạng neural, chúng ta sẽ biểu diễn các nhãn bằng các vector chỉ gồm giá trị 0 và 1.\n",
    "\n",
    "$$y=\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\end{array}\\right],\\text{   }\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\end{array}\\right],\\text{   } \\cdots \\text{    or    }\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\\\\\n",
    "\\end{array}\\right]\\text{ .}\n",
    "$$\n",
    "\n",
    "Ví dụ, nếu $x^{(i)}$ là ảnh của chữ số 5, giá trị $y^{(i)}$ tương ứng là một vector 10 chiều với $y_5=1$ và các phần tử khác bằng 0.\n",
    "\n",
    "### 1.4 Regularized cost function\n",
    "\n",
    "Hàm cost cho mạng neural với **regularization** được định nghĩa như sau:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) & = &\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\left[-y_k^{(i)}\\log((h_\\theta(x^{(i)}))_k)-(1-y_k^{(i)}) \\log(1-(h_\\theta(x^{(i)}))_k) \\right] + \\\\\n",
    "& & \\frac{\\lambda}{2m} \\left[ \\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\Theta_{j,k}^{(1)})^2 +\n",
    "\\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\Theta_{j,k}^{(2)})^2 \\right] & \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Các chú ý khi cài đặt cost function.\n",
    "\n",
    "- Cài đặt nên tổng quát cho số lượng unit bất kỳ của các layer.\n",
    "- Không thực hiện regularization cho số hạng tương ứng với bias.\n",
    "\n",
    "Hàm sau sẽ cài đặt regularized cost function. Khi muốn sử dụng phiên bản non-regularizaton, chỉ cần cho giá trị $\\lambda=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # z can be np.ndarray, np.matrix, or scalar\n",
    "    return 1 / ( 1 + np.exp(-z) ) \n",
    "\n",
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size,\n",
    "                     num_labels, X, y, _lambda):\n",
    "    \"\"\" Tính cost function cho neural networks\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    nn_params : ndarray\n",
    "             unrolled vector of theta1 and theta2\n",
    "    theta1 : ndarray\n",
    "             Tham số của mạng neural cho tầng ẩn (tầng thứ 2)\n",
    "    theta2 : ndarray\n",
    "             Tham số của mạng neural cho tầng output (tầng thứ 3)\n",
    "    input_layer_size : int\n",
    "    hidden_layer_size : int\n",
    "    num_labels : int\n",
    "    X      : ndarray\n",
    "             Ma trận trong đó mỗi hàng lưu các features của các examples\n",
    "    y      : ndarray\n",
    "             Danh sách nhãn của các examples\n",
    "    _lambda : float\n",
    "             Tham số trong công thức regularization    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    f : float\n",
    "        Giá trị của cost function trong mạng neural\n",
    "    \n",
    "    Notes\n",
    "    -----------\n",
    "    Kích thước của ma trận trong dữ liệu được cung cấp\n",
    "    theta1 :  25 x 401\n",
    "    theta2 :  10 x 26\n",
    "    X : 5000 x 400\n",
    "    y : (5000, ) ndim = 1\n",
    "    \"\"\"    \n",
    "    m = X.shape[0]\n",
    "    theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape( \n",
    "                             (hidden_layer_size, input_layer_size + 1))    \n",
    "    \n",
    "    theta2 = nn_params[hidden_layer_size * (input_layer_size+1):].reshape(\n",
    "                            (num_labels, hidden_layer_size+1))\n",
    "        \n",
    "    X_n = np.concatenate( ( np.ones((m,1)), X ), axis=1 )\n",
    "    Z2 = np.dot( X_n, theta1.T ) # 5000 x 25\n",
    "    A2 = sigmoid(Z2)\n",
    "    A2 = np.concatenate( ( np.ones((m,1)), A2 ), axis=1 )\n",
    "    Z3 = np.dot( A2, theta2.T ) # 5000 * 10\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    # TASK: Cải tiến cài đặt dưới đây -- chỉ dùng 1 vòng lặp trên các examples\n",
    "    J = 0    \n",
    "    for i in xrange(m):\n",
    "        for k in xrange(1, num_labels+1):\n",
    "            yk = np.int(y[i]==k)\n",
    "            cost = -yk * np.log( A3[i,k-1] ) - (1-yk) * np.log( 1 - A3[i,k-1] )\n",
    "            J += cost\n",
    "    J /= m\n",
    "    \n",
    "    # Regularized version of cost function \n",
    "    J += _lambda * ( (theta1[:,1:] ** 2).sum() + (theta2[:,1:] ** 2).sum() ) / (2*m)\n",
    "    \n",
    "    return J    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ sử dụng hàm ```nn_cost_function``` để tính cost function với dữ liệu và tham số đã cho. Trước đó, chúng ta cần sinh \"unrolled\" vector gồm các phần tử trong $\\Theta_1$ và $\\Theta_2$, và khởi tạo các tham số cần thiết cho hàm số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "nn_params = np.concatenate( (Theta1.flatten(), Theta2.flatten()))\n",
    "input_layer_size  = X.shape[1]\n",
    "hidden_layer_size = Theta1.shape[0]\n",
    "num_labels = Theta2.shape[0]\n",
    "\n",
    "_lambda = 0\n",
    "J = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels,\n",
    "                     X, y, _lambda)\n",
    "print('Cost at parameters (loaded from ex4weights): %f \\n'\n",
    "      '(this value should be about 0.287629)' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính cost function với phiên bản regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Cost Function (with Regularization) ...\n",
      "Cost at parameters (loaded from ex4weights): 0.383770 \n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "print 'Checking Cost Function (with Regularization) ...'\n",
    "_lambda = 1 \n",
    "J = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels,\n",
    "                     X, y, _lambda)\n",
    "print('Cost at parameters (loaded from ex4weights): %f \\n'\n",
    "      '(this value should be about 0.383770)' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation\n",
    "\n",
    "Trong phần này, chúng ta sẽ cài đặt thuật toán **backpropagation** để tính gradient cho cost function của neural networks.\n",
    "\n",
    "### 2.1 Sigmoid gradient\n",
    "\n",
    "Để tính gradient, trước tiên chúng ta sẽ cài đặt hàm sigmoid gradient. Gradient của sigmoid function được tính như sau:\n",
    "\n",
    "$$g^{\\prime}(z)=\\frac{d}{dz}g(z)=g(z)(1-g(z))$$\n",
    "\n",
    "Ở đây $g(z)$ là sigmoid function.\n",
    "\n",
    "$$sigmoid(z)=g(z)=\\frac{1}{1+e^{-z}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    # z can be scalar or np.ndarray\n",
    "    g = sigmoid(z)\n",
    "    return g * (1-g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ thử nghiệm hàm đã cài đặt ở trên với một số giá trị của $z$. Khi giá trị rất $z$ rất lớn, hàm số sẽ dần tới 0. Khi $z=0$, hàm số sẽ có giá trị bằng 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_gradient(10000) = 0.0000\n",
      "sigmoid_gradient(0) = 0.2500\n",
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "for z in [10000, 0]:\n",
    "    print 'sigmoid_gradient(%.0f) = %.4f' % (z, sigmoid_gradient(z))\n",
    "    \n",
    "g = sigmoid_gradient( np.array([1, -0.5, 0, 0.5, 1]))\n",
    "print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random initialization\n",
    "\n",
    "Khi huấn luyện mạng neural, chúng ta cần khởi tạo các giá trị tham số ban đầu. Khác với thuật toán gradient descent, chúng ta không thể sử dụng zero initialization (nếu sử dụng các tham số tương ứng với các unit ở các tầng sẽ bằng nhau, kết quả là với mỗi ví dụ, mạng neural thu được sẽ trả về xác suất bằng nhau đối với mọi lớp). Để tránh điều này, chúng ta sẽ khởi tạo các tham số ban đầu một cách ngẫu nhiên theo phân bố uniform trong khoảng $[-\\epsilon_{init},\\epsilon_{init}]$. Chúng ta sẽ chọn $\\epsilon_{init}=0.12$. Cài đặt sau sẽ thực hiện công việc đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_initialize_weights(L_in, L_out):\n",
    "    \"\"\" Khởi tạo ngẫu nhiên các tham số ban đầu cho một tầng\n",
    "    Parameters\n",
    "    -----------\n",
    "    L_in : int\n",
    "           Số connection đầu vào của tầng\n",
    "    L_out : int\n",
    "           Số connection đầu ra của tầng\n",
    "           \n",
    "    Return\n",
    "    ----------\n",
    "    W : np.float64\n",
    "        Ma trận với kích thước L_out x (1 + L_in) gồm các số ngẫu nhiên trong khoảng\n",
    "        [-epsilon, -epsilon]\n",
    "    \"\"\"\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.randn(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng hàm trên, chúng ta sẽ khởi tạo các giá trị tham số ban đầu cho neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(99999)\n",
    "initial_Theta1 = rand_initialize_weights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = rand_initialize_weights(hidden_layer_size, num_labels)\n",
    "# unroll parameters\n",
    "initial_nn_params = np.concatenate( (initial_Theta1.flatten(), \n",
    "                                     initial_Theta2.flatten()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backpropagation\n",
    "\n",
    "<img src=\"backpropagation_updates.png\" alt=\"Drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "Phần này sẽ hướng dẫn các bạn cài đặt thuật toán **backpropagation**. Ý tưởng chính của thuật toán **backpropagation** như sau. Với một training example $(x^{(t)},y^{(t)})$ cho trước, chúng ta sẽ thực hiện bước \"forward pass\" để tính tất các giá trị **activation** qua mạng neural, bao gồm cả các giá trị đầu ra của hypothesis $h_\\theta(x)$. Sau đó, tại mỗi node $j$ ở tầng $l$, chúng ta sẽ tính \"error term\" $\\delta_j^{(l)}$. \"Error term\" đánh giá xem một node sẽ chịu đóng góp như thế nào đối với bất kỳ lỗi nào của đầu ra.\n",
    "\n",
    "Tại các node ở tầng output, chúng ta có thể tính trực tiếp sự khác biệt giữa đầu ra của mạng neural với giá trị thực tế, và sử dụng tính toán đó để định nghĩa $\\delta_j^{(3)}$ (bởi vì tầng thứ 3 là tầng output). Với các tầng ẩn, chúng ta sẽ tính $\\delta_j^{(l)}$ dựa trên giá trị trung bình trọng số của các \"error term\" của các node ở tầng $l+1$. Xem mô tả chi tiết thuật toán **backpropagation** tại [ex4.pdf](https://github.com/minhpqn/Machine-Learning-Dojo/blob/master/code/neural_networks_learning/ex4.pdf)\n",
    "\n",
    "### 2.4 Regularized Neural Networks\n",
    "\n",
    "Sau khi đã hoàn thành cài đặt thuật toán backpropagation, chúng ta sẽ cài đặt phiên bản regularization cho gradient. Chúng ta thực hiện việc này bằng cách cộng thêm các số hạng bổ sung sau khi tính gradients sử dụng thuật toán backpropagation. \n",
    "\n",
    "Đặc biệt, sau khi tính $\\Delta_{ij}^{(l)}$ sử dụng thuật toán backpropagation, chúng ta cộng thêm số hạng regularization như sau:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} \\quad \\quad \\text{với } j=0$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m}\\Theta_{ij}^{(l)} \\quad \\quad \\text{với } j \\geq 0$$\n",
    "\n",
    "Chúng ta sẽ không thực hiện regularization cho cột đầu tiên của $\\Theta^{(l)}$ vì nó được sử dụng cho các *bias term*. Hơn thế, trong các tham số $\\Theta_{ij}^{(l)}$, chỉ số $i$ bắt đầu từ 1 và $j$ bắt đầu từ 0. Vì thế:\n",
    "\n",
    "$$\\Theta^{(l)}=\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\Theta_{1,0}^{(l)} & \\Theta_{1,1}^{(l)} & \\cdots\\\\\n",
    "\\Theta_{2,0}^{(l)} & \\Theta_{2,1}^{(l)} & \\quad\\\\\n",
    "\\vdots & \\quad & \\ddots\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Code dưới đây sẽ cài đặt thuật toán **backpropagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels,\n",
    "                     X, y, _lambda):\n",
    "    \"\"\" Tính gradient cho cost function trong neural networks\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    nn_params : ndarray\n",
    "             unrolled vector of theta1 and theta2\n",
    "    theta1 : ndarray\n",
    "             Tham số của mạng neural cho tầng ẩn (tầng thứ 2)\n",
    "    theta2 : ndarray\n",
    "             Tham số của mạng neural cho tầng output (tầng thứ 3)\n",
    "             \n",
    "    input_layer_size : int\n",
    "    hidden_layer_size : int\n",
    "    num_labels : int\n",
    "    \n",
    "    X      : ndarray\n",
    "             Ma trận trong đó mỗi hàng lưu các features của các examples\n",
    "    y      : ndarray\n",
    "             Danh sách nhãn của các examples\n",
    "    _lambda : float\n",
    "             Tham số trong công thức regularization    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    grad : ndarray\n",
    "        unrolled vector của gradient cho mỗi tầng\n",
    "    \n",
    "    Notes\n",
    "    -----------\n",
    "    Kích thước của ma trận trong dữ liệu được cung cấp\n",
    "    theta1 :  25 x 401\n",
    "    theta2 :  10 x 26\n",
    "    X : 5000 x 400\n",
    "    y : (5000, ) ndim = 1\n",
    "    \"\"\"    \n",
    "    \n",
    "    m = X.shape[0]\n",
    "    theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape( \n",
    "                             (hidden_layer_size, input_layer_size + 1))    \n",
    "    \n",
    "    theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].reshape(\n",
    "                            (num_labels, hidden_layer_size + 1))\n",
    "    \n",
    "    theta1_grad = np.zeros( theta1.shape )\n",
    "    theta2_grad = np.zeros( theta2.shape )\n",
    "        \n",
    "    X_n = np.concatenate( ( np.ones((m,1)), X ), axis=1 )\n",
    "    Z2 = np.dot( X_n, theta1.T ) # 5000 x 25\n",
    "    A2 = sigmoid(Z2)\n",
    "    # A2.shape now -> m x (1+hidden_layer_size)\n",
    "    A2 = np.concatenate( ( np.ones((m,1)), A2 ), axis=1 )\n",
    "    Z3 = np.dot( A2, theta2.T ) # 5000 * 10\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    c = np.array( range(1, num_labels + 1) )\n",
    "    for t in xrange(m):\n",
    "        yt = y[t]\n",
    "        # Step 1: Perform feedforward pass computing the activations\n",
    "        # z1, a2, z3, a3 (Already done)\n",
    "        a1 = X_n[[t],:] # 1 x 401 -- ugly indexing\n",
    "        z2 = Z2[[t],:]\n",
    "        a2 = A2[[t],:]\n",
    "        z3 = Z3[[t],:]\n",
    "        a3 = A3[[t],:] # shape = 1 x 10        \n",
    "        \n",
    "        # Step 2: Compute error terms for output layer\n",
    "        # For each ouput unit k in layer 3 compute delta3[k]\n",
    "        \n",
    "        delta_3 = a3 - ( c == yt ) # shape 1 x 10\n",
    "        \n",
    "        # Step 3: Compute error terms for layer 2\n",
    "        # delta_2 = np.dot(delta_3, theta2) * [ 1 sigmoid_gradient(z2) ]\n",
    "        sigmoid_grad_z2 = np.concatenate((np.ones( (1,1) ), \n",
    "                                         sigmoid_gradient(z2) ),\n",
    "                                         axis=1)\n",
    "        delta_2 = np.dot(delta_3, theta2) * sigmoid_grad_z2 # shape = 1 x 26\n",
    "        \n",
    "        # Step 4: Accumulate the gradient from this example\n",
    "        theta1_grad = theta1_grad + np.dot( delta_2[:,1:].T, a1 )\n",
    "        theta2_grad = theta2_grad + np.dot( delta_3.T, a2 )\n",
    "        \n",
    "    theta1_grad /= m\n",
    "    theta2_grad /= m\n",
    "    # regularized version for gradient\n",
    "    theta1_grad[:,1:] += _lambda * theta1[:,1:]/m\n",
    "    theta2_grad[:,1:] += _lambda * theta2[:,1:]/m\n",
    "    # unrolled gradients\n",
    "    grad_ = np.concatenate( ( theta1_grad.flatten(), \n",
    "                             theta2_grad.flatten() ) )\n",
    "      \n",
    "    return grad_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Gradient checking\n",
    "\n",
    "Trong khi huấn luyện một mạng neural, chúng ta sẽ tìm các tham số để cực tiểu hoá hàm cost $J(\\Theta)$. Đeer thực hiện gradient checking trên các giá trị tham số, bạn có thể tưởng tượng chúng ta \"unroll\" các tham số $\\Theta^{(1)}$, $\\Theta^{(2)}$ thành một vector duy nhất $\\theta$. Hàm cost sẽ trở thành hàm $J(\\theta)$ của **unrolled** vector. Nhờ đó chúng ta có thể thực hiện gradient checking như sau.\n",
    "\n",
    "Giả sử chúng ta có hàm số $f_i(\\theta)$ để tính giá trị $\\frac{\\partial}{\\partial \\theta_i}J(\\theta)$ và bạn muốn kiểm tra xem liệu $f_i$ có trả về các giá trị đạo hàm chính xác hay không.\n",
    "\n",
    "$$\\text{Đặt}\\quad\n",
    "\\theta^{(i+)}=\\theta+\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "\\epsilon\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\end{array}\\right]\n",
    "\\quad\\text{ và }\\quad\n",
    "\\theta^{(i+)}=\\theta-\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "\\epsilon\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$\\theta^{(i+)}$ giống hệt $\\theta$ ngoại trừ phần tử thứ $i$-th được cộng thêm $\\epsilon$. Tương tự như vậy $\\theta^{(i-)}$ là vector với phần tử thứ $i$-th bị giảm đi $\\epsilon$. Bây giờ bạn có thể đánh giá tính chính xác $f_i(\\theta$ bằng cách kiểm tra, liệu với mỗi giá trị của $i$, điều kiện sau có thoả mãn không:\n",
    "\n",
    "$$f_i(\\theta)\\approx \\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})}{2\\epsilon}$$\n",
    "\n",
    "Mức độ xấp xỉ của hai giá trị trong công thức trên phụ thuộc vào hàm cost $J$. Nhưng giả sử $\\epsilon=10^{-4}$, bạn sẽ luôn thấy vế trái và vế phải trong công thức trên sẽ giống nhau tới 4 chữ số sau phần thập phân (thường là nhiều hơn).\n",
    "\n",
    "Đoạn code dưới đây sẽ cài đặt numerical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_numerical_gradient(J, theta):\n",
    "    \"\"\" Tính numerical gradient theo công thức trên\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    J : wrapper function cho cost function. Hàm này nhận đối số là tham số theta\n",
    "    theta: numpy.ndarray\n",
    "        unrolled vector chứa các tham số của mạng neural\n",
    "        \n",
    "    Returns\n",
    "    -------------\n",
    "    numgrad : numpy.ndarray\n",
    "       Giá trị của hàm gradient (numerical version)\n",
    "    \"\"\"\n",
    "    numgrad = np.zeros( theta.shape )    \n",
    "    perturb = np.zeros( theta.shape )\n",
    "    e = 1e-4\n",
    "    for p in range( len(theta) ):\n",
    "        # Set perturbation vector\n",
    "        perturb[p] = e\n",
    "        loss1 = J(theta - perturb)\n",
    "        loss2 = J(theta + perturb)\n",
    "        # Compute numerical gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "        perturb[p] = 0\n",
    "        \n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm dưới đây sẽ tạo ra một mạng neural với kích thước nhỏ và kiểm tra các giá trị backpropagation gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "    \"\"\"\n",
    "    initializes the weights \n",
    "    of a layer with fan_in incoming connections and fan_out outgoing \n",
    "    connections using a fix set of values\n",
    "    Note that W should be set to a matrix of size(1 + fan_in, fan_out) as\n",
    "    the first row of W handles the \"bias\" terms\n",
    "    \"\"\"\n",
    "    W = np.zeros( (fan_out, 1 + fan_in) )\n",
    "    W = np.sin( range(1, W.size+1) ).reshape(W.shape)/10\n",
    "    return W  \n",
    "    \n",
    "def checkNNGradients(_lambda=0):\n",
    "    \"\"\" Tạo một mạng neural cỡ nhỏ để kiểm tra backpropagation gradients\n",
    "    Returns\n",
    "    -------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "\n",
    "    X = debugInitializeWeights(m, input_layer_size -1)\n",
    "    y = 1 + np.mod( range(1,m+1), num_labels)\n",
    "    nn_params = np.concatenate( ( theta1.flatten(), theta2.flatten()) )\n",
    "    costFunc = lambda p: nn_cost_function(p, input_layer_size, \n",
    "                                          hidden_layer_size, num_labels,\n",
    "                                          X, y, _lambda)\n",
    "    \n",
    "    gradFunc = lambda p: nn_gradient(p, input_layer_size, \n",
    "                                     hidden_layer_size, num_labels,\n",
    "                                     X, y, _lambda)\n",
    "    \n",
    "    grad = gradFunc(nn_params)\n",
    "    numgrad = compute_numerical_gradient(costFunc, nn_params)\n",
    "      \n",
    "    print 'The above two columns you get should be very similar.'\n",
    "    print '(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n'\n",
    "    \n",
    "    # Evaluate the norm of the difference between two solutions.\n",
    "    # If you have a correct implementation, and assuming you used \n",
    "    # EPSILON = 0.0001 in compute_mumerical_gradient(),\n",
    "    # then diff below should be less than 1e-9\n",
    "    \n",
    "    diff = np.linalg.norm(numgrad-grad)/np.linalg.norm(numgrad+grad)\n",
    "    \n",
    "    print 'If your backpropagation implementation is correct, then'\n",
    "    print 'the relative difference will be small (less than 1e-9).'\n",
    "    print 'Relative Difference: %g' % diff    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ sử dụng hàm cài đặt ở trên để kiểm tra xem cài đặt thuật toán backpropagation gradients đã chính xác chưa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Backpropagation...\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then\n",
      "the relative difference will be small (less than 1e-9).\n",
      "Relative Difference: 2.07115e-11\n",
      "\n",
      "Checking Backpropagation (w/ Regularization)\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then\n",
      "the relative difference will be small (less than 1e-9).\n",
      "Relative Difference: 2.025e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3): 0.576051\n",
      "\n",
      "(this value should be about 0.576051)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Checking Backpropagation...'\n",
    "checkNNGradients()\n",
    "print '\\nChecking Backpropagation (w/ Regularization)'\n",
    "_lambda = 3\n",
    "checkNNGradients(_lambda)\n",
    "debug_J = nn_cost_function(nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, _lambda);\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = 3): %f\\n'\n",
    "      '\\n(this value should be about 0.576051)\\n' % debug_J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Học tham số với fmin_l_bfgs_b\n",
    "\n",
    "Sau khi cài đặt xong hàm cost và gradient cho mạng neural, trong phần tiếp theo, chúng ta sẽ học các tham số sử dụng thuật toán tối ưu hoá L-BFGS-B. Trong python, chúng ta sẽ dùng hàm [fmin_l_bfgs_b](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html).\n",
    "\n",
    "Sau khi huấn luyện xong mạng neural, tôi sẽ tính độ chính xác của dự đoán trên dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network...\n",
      "\n",
      "Training accuracy (with neural networks): 95.58\n"
     ]
    }
   ],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0]\n",
    "    X_new = np.concatenate( (np.ones( (m,1) ), X), axis=1 ) # 5000 x 401\n",
    "    A2 = sigmoid( np.dot(X_new, Theta1.T) ) # 5000 x 25\n",
    "    A2 = np.concatenate( ( np.ones( (m,1) ), A2 ), axis=1 ) # 5000 x 26\n",
    "    A3 = sigmoid( np.dot(A2, Theta2.T) ) # 5000 x 10\n",
    "    p = np.argmax( A3, axis=1 ) + 1\n",
    "    return p\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "_lambda = 1\n",
    "input_layer_size  = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "print 'Training Neural Network...'\n",
    "nn_params, f, d = fmin_l_bfgs_b(nn_cost_function, initial_nn_params,\n",
    "                          fprime=nn_gradient, \n",
    "                          args=(input_layer_size, \n",
    "                                hidden_layer_size, \n",
    "                                num_labels,\n",
    "                                X, y, _lambda),\n",
    "                          disp=False, maxiter=50)\n",
    "\n",
    "Theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape(\n",
    "                           (hidden_layer_size, input_layer_size + 1))    \n",
    "Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].reshape(\n",
    "                            (num_labels, hidden_layer_size + 1))\n",
    "\n",
    "pred = predict(Theta1, Theta2, X)\n",
    "print('\\nTraining accuracy (with neural networks): %s' % \n",
    "      ( 100 * (pred == y).mean() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Còn 3 phần trong bài tập này tôi chưa hoàn thành:\n",
    "\n",
    "- Visualizing data\n",
    "- Hiển thị tầng ẩn\n",
    "- Thay đổi các tham số $\\lambda$ và số lượng vòng lặp và quan sát sự thay đổi của kết quả thu được.\n",
    "\n",
    "Bạn có thể xem cài đặt chi tiết của bài tập này trong một script duy nhất tại [đây](https://github.com/minhpqn/Machine-Learning-Dojo/blob/master/code/neural_networks_learning/Neural_Networks_Learning.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
