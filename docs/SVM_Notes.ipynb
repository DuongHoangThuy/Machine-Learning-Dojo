{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Notes\n",
    "\n",
    "Những ghi chép cá nhân khi học về thuật toán Support Vector Machines\n",
    "\n",
    "## Một số khái niệm\n",
    "\n",
    "### Functional margin là gì\n",
    "\n",
    "Trong SVM, siêu phẳng được xác định bởi các tham số $(w,b)$. Với một training example $x^{(i)},y^{(i)}$, **functional margin** được định nghĩa như sau.\n",
    "\n",
    "$$\\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx+b)$$\n",
    "\n",
    "Khi trị tuyệt đối của khoảng cách từ training example tới siêu phẳng càng lớn thì độ tin cậy của dự đoán càng cao. Chú ý $y^{(i)}$ nhận giá trị trong tập $\\{1,-1\\}$.\n",
    "\n",
    "### Bài toán tối ưu hoá trong trường hợp dữ liệu có thể phân chia tuyến tính được\n",
    "\n",
    "Trường hợp này được gọi là **hard margin**.\n",
    "\n",
    "$$\\text{Minimize }\\quad \\|\\vec{w}\\|\\quad \\text{subject to}\\quad y_i(\\vec{w}\\cdot\\vec{x_i} - b) \\ge 1\\quad \\text{for}\\quad i = 1,\\,\\ldots,\\,n$$\n",
    "\n",
    "### Bài toán tối ưu hoá trong trường hợp dữ liệu không phân chia tuyến tính được\n",
    "\n",
    "Để mở rộng SVM với dữ liệu không thể phân chia tuyến tính được, *hinge loss function* được đưa ra.\n",
    "\n",
    "$$\\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x_i} + b)\\right)$$\n",
    "\n",
    "Trong trường hợp này, ta cần cực tiểu hoá hàm sau đây:\n",
    "\n",
    "$$\\left[\\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(w\\cdot x_i + b)\\right) \\right] + \\lambda\\lVert w \\rVert^2. \\qquad(1)$$\n",
    "\n",
    "Dạng bài toán tối ưu với trường hợp lề mềm:\n",
    "\n",
    "$$\\text{minimize } \\frac 1 n \\sum_{i=1}^n \\zeta_i + \\lambda\\|w\\|^2$$\n",
    "$$\\text{subject to } y_i(x_i \\cdot w + b) \\geq 1 - \\zeta_i \\,\\text{ and }\\,\\zeta_i \\geq 0,\\,\\text{for all }i.$$\n",
    "\n",
    "Trong đó $\\zeta_i = \\max\\left(0, 1 - y_i(w\\cdot x_i + b)\\right)$\n",
    "\n",
    "### Ý nghĩa của tham số $\\lambda$\n",
    "\n",
    "Tham số $\\lambda$ xác định độ ưu tiên giữa đảm bảo lề đủ rộng (mô hình tổng quát hơn trên dữ liệu *unseen*), và và **training examples** nằm ở nửa mặt phẳng tương ứng với nhãn của nó (mô hình *fit* với dữ liệu huấn luyện).\n",
    "Nếu tham số $\\lambda$ nhỏ, thuật toán SVM sẽ ưu tiên việc *fit* dữ liệu, do đó mô hình học được có thể không được tổng quát cho lắm (trường hợp này được gọi là **overfitting**). Ngược lại nếu $\\lambda$ lớn, thuật toán ưu tiên việc đảm bảo lề đủ rộng, do đó mô hình có thể không được fit với dữ liệu huấn luyện (trường hợp này được gọi là **underfitting**).\n",
    "\n",
    "Trong các SVM library, tham số $C$ tương đương với $1/\\lambda$. Do đó nếu muốn mô hình fit với dữ liệu huấn luyện, chúng ta sẽ chọn $C$ có giá lớn; ngược lại nếu muốn lề đủ rộng (mô hình tổng quát hơn), chúng ta sẽ chọn $C$ có giá trị nhỏ hơn.\n",
    "\n",
    "### Support Vectors là gì\n",
    "\n",
    "Là những điểm nằm trên lề (margin) của hyperplane. Những điểm này thoả mãn hai phương trình sau đây:\n",
    "\n",
    "$$\\vec{w}\\vec{x} - b = 1$$\n",
    "\n",
    "hoặc:\n",
    "\n",
    "$$\\vec{w}\\vec{x} - b = -1$$\n",
    "\n",
    "## Những chú ý khi sử dụng SVM (SVM Best Practices)\n",
    "\n",
    "### Chọn tham số trong SVM\n",
    "\n",
    "Trích từ slide bài giảng trên Machine Learning Course (Coursera). Tham số $C=\\frac{1}{\\lambda}$, trong đó $\\lambda$ là regularization parameter (đã học trong các bài giảng trước). Khi giá trị $C$ càng lớn, tức là $\\lambda$ càng nhỏ, mô hình sẽ **fit** dữ liệu tốt hơn, nhưng dễ bị **overfit**, do đó ta nói mô hình \"lower bias, high variance\". Ngược lại khi $C$ càng nhỏ, tức là $\\lambda$ càng lớn, mô hình sẽ **fit** dữ liệu kém hơn, nhưng dễ bị **underfit**, do đó ta nói mô hình có đặc điểm \"higher bias, low variance\".\n",
    "\n",
    "<img src=\"SVM_Parameters.png\" alt=\"Drawing\" style=\"width:700px;\"/>\n",
    "\n",
    "### SVM Practical usage and tips\n",
    "\n",
    "Copy trên [Website của sklearn](http://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use).\n",
    "\n",
    "<img src=\"Practical_usage.png\" alt=\"Drawing\" style=\"width:700px;\"/>\n",
    "\n",
    "## Thư viện SVM trong python\n",
    "\n",
    "- [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) - python wrapper của thư viện [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/). Tốc độ khá chậm.\n",
    "- [sklearn.svm.LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) - dựa trên thư viên [LibLinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/). SVM với linear kernel, tốc độ khá nhanh, thích hợp khi học trên dữ liệu lớn.\n",
    "\n",
    "## Tài liệu tham khảo\n",
    "\n",
    "- [Support-vector-machines.org](http://www.support-vector-machines.org/)\n",
    "- [http://www.kernel-machines.org](http://www.kernel-machines.org/)\n",
    "- [Bài viết trên Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine) về support vector machines.\n",
    "- [The Simplified SMO algorithm](http://cs229.stanford.edu/materials/smo.pdf)\n",
    "- Platt. [Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines](http://research.microsoft.com/pubs/69644/tr-98-14.pdf)\n",
    "- Bài viết trên Wikipedia về Sequential Minimal Optimization algorithm: [https://en.wikipedia.org/wiki/Sequential_minimal_optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization)\n",
    "- [SVM Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) của khoá học CS229."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
